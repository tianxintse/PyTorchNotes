{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络\n",
    "\n",
    "在pytorch中，可以使用`torch.nn`包来构建神经网络。\n",
    "\n",
    "现在我们已经了解什么是`autograd`，`nn`包依赖于`autograd`去完成建模以及求微分。一个`nn.Module`包含了神经网络中的层，它还有一个方法`forware(input)`用于获得模型的输出。\n",
    "\n",
    "下面是一个用于分类手写数字的网络：\n",
    "![mnist](./images/003/mnist.png)\n",
    "<center>图1.1 LeNet</center>\n",
    "\n",
    "上面是一个很简单的前馈网络。它获得输入，然后沿着网络层往前传递，最后得到输出。\n",
    "\n",
    "训练一个神经网络的主要步骤如下：\n",
    "\n",
    "- 定义一个具有可学习参数(或称为权重)的神经网络\n",
    "- 开始在数据集上迭代\n",
    "- 将数据输入网络\n",
    "- 计算loss\n",
    "- 将loss反向传播\n",
    "- 更新可学习的网络参数，通常使用算法：`weight = weight - learning_rate * gradient`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义神经网络\n",
    "\n",
    "下面就来定义一个神经网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在定义网络时，我们只需定义前向传播的过程，例如上面的`forward`函数，反向传播会由`autograd`包自动计算。在`forward`函数中你可以对`Tensor`进行任何操作。\n",
    "\n",
    "可使用`net.parameters()`获得可学习的网络参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来试一下 $32 \\times 32$ 的随机输入。**注意：此网络(LeNet)的输入大小为 $32 \\times 32$，如果要使用MNIST数据集，需要将图片大小转换为 $32 \\times 32$。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0675,  0.1205,  0.0065,  0.0294,  0.0679, -0.0201, -0.0432,  0.0713,\n",
      "          0.0204, -0.0401]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清除网络中的梯度缓存，然后使用一组随机值进行反向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e6e0f5;padding:10px;border-radius:5px;\">\n",
    "<span style=\"font-size:14px;color:white;background-color:#0ccbea;padding:5px;display:block;\">NOTE</span>\n",
    "\n",
    "`torch.nn` only supports mini-batches. The entire `torch.nn` package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "\n",
    "For example, `nn.Conv2d` will take in a 4D Tensor of `nSamples x nChannels x Height x Width`.\n",
    "\n",
    "If you have a single sample, just use `input.unsqueeze(0)` to add a fake batch dimension.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回顾：**\n",
    "\n",
    "- `torch.Tensor` - 一个多维数组，支持自动微分，并且在自动微分过程中保存关于张量的梯度。\n",
    "- `nn.Module` - 神经网络模型，便于封装模型，而且有预定义方法支持移动到GPU计算，导出导入模型等。\n",
    "- `nn.Parameter` - 一个`Tensor`，用于定义模型的参数(例如`weight`或`bias`)，定义式会自动加入到`Module`中。\n",
    "- `autograd.Function` - 实现了一些预定义函数(例如`relu`)的前向和反向传播过程。\n",
    "\n",
    "任何一个张量运算都会创建一个`Function`节点，并且这个节点会创建一个输出张量，张量运算也会被记录到模型的运算历史中。\n",
    "\n",
    "**目前已学习的内容**：\n",
    "\n",
    "- 定义网络\n",
    "- 输入并反向传播\n",
    "\n",
    "**剩余内容**：\n",
    "\n",
    "- 计算loss\n",
    "- 迭代更新网络的权重参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss 函数\n",
    "\n",
    "Loss函数接收`(output,target)`输入对，然后计算出模型输出和目标值的偏离程度。\n",
    "\n",
    "`nn`包中有几种不同的[loss函数](https://pytorch.org/docs/stable/nn.html)。例如最简单的`nn.MSELoss`计算`output`和`target`之间的平方误差。\n",
    "\n",
    "例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7537, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)\n",
    "target = target.view(1, -1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，如果跟踪loss的反向传播过程，通过它的`.grad_fn`属性，可以看到类似如下的计算图：\n",
    "```\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "```\n",
    "\n",
    "当我们调用`loss.backward()`的时候，整个计算图的每个节点都会计算关于`loss`的微分。计算图中所有属性`requires_grad=True`的`Tensor`，它们的`.grad`属性中都会累积关于`loss`的梯度。\n",
    "\n",
    "下面是回溯了几步的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x0000029087F191D0>\n",
      "<AddmmBackward object at 0x0000029087F71908>\n",
      "<AccumulateGrad object at 0x0000029087F719E8>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播\n",
    "\n",
    "进行反向传播很简单，只需调用`loss.backward()`。在调用之前需要清除已经累积的梯度，否则新计算的梯度会累加到已经存在的梯度当中。\n",
    "\n",
    "现在调用`loss.backward()`，然后观察`conv1`的`bias`的梯度有什么变化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0023,  0.0104, -0.0071,  0.0076, -0.0022,  0.0197])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network package contains various modules and loss functions that form the building blocks of deep neural networks. A full list with documentation is [here](https://pytorch.org/docs/stable/nn.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更新网络参数\n",
    "\n",
    "最简单的方法是随机梯度下降(Stochastic Gradient Descent, SGD)：\n",
    "\n",
    "$$\n",
    "\\text{weight} = \\text{weight} - \\text{learning_rate} * \\text{gradient}\n",
    "$$\n",
    "\n",
    "简单实现：\n",
    "```python\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "```\n",
    "\n",
    "但是框架有很多内置的优化方法，例如SGD, Nesterov-SGD, Adam, RMSProp等等。我们不需要自己写，`torch.optim`中包含了所有的优化方法，使用很简单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建优化器\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# 一次迭代\n",
    "optimizer.zero_grad()               # 清除梯度缓存\n",
    "output = net(input)\n",
    "loss = criterion(output, target)    # 计算loss\n",
    "loss.backward()                     # 反向传播\n",
    "optimizer.step()                    # 更新权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e6e0f5;padding:10px;border-radius:5px;\">\n",
    "<span style=\"font-size:14px;color:white;background-color:#0ccbea;padding:5px;display:block;\">NOTE</span>\n",
    "Observe how gradient buffers had to be manually set to zero using `optimizer.zero_grad()`. This is because gradients are accumulated as explained in <a href=\"https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#backprop\">Backprop</a> section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
